---
title: "Bayesian ROS model"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(stringr)
library(here)

library(cmdstanr)
library(posterior)

library(ggplot2)
theme_set( theme_bw() )

MODEL_DIR <- here("stan_models")
if (!dir.exists(MODEL_DIR)) dir.create(MODEL_DIR)

```

## Bayesian ROS model step by step

This doc loads ROS and weather data, and then builds a Bayesian ROS model step by step.


## Helper code

A utility function for sampling from cmdstanr models. This was provided in Richard McElreath's workshop materials (`script.R`) but has been slightly tweaked here to store the Stan model in specified directory and with a file name based on the name of the variable passed to the `model_code` argument. 

```{r}

get_samples <- function(model_code, data=list(), 
                        model_dir = MODEL_DIR, 
                        model_basename = NULL,
                        seed=123, chains=4, refresh = 0,
                        vars = NULL) {
  
  if (is.null(model_basename)) {
    model_basename <- deparse(substitute(model_code))
    
    # remove '_code' from the base name if present
    model_basename <- sub(model_basename, pattern = "_code", replacement = "")
  }
  
  f <- write_stan_file(model_code, 
                       dir = model_dir, 
                       basename = model_basename)
  
  model_x <- cmdstan_model(f)
  samples_1 <- model_x$sample(
    data = data,
    seed = seed,
    chains = chains,
    parallel_chains = chains,
    refresh = refresh
  )
  
  if (is.null(vars)) {
    # Get all variables from the fitted model
    pr <- as_draws_rvars( samples_1$draws() )
  } else {
    # Get specified variables
    pr <- as_draws_rvars( samples_1$draws(variables = vars))
  }
  p <- list()
  for ( i in 1:length(pr) )
    p[[ names(pr)[i] ]] <- draws_of( pr[[i]] )
  return(p)
}

```




## ROS and weather

Load the ROS and weather data. Select a couple of weather variables to use as predictors for an initial model. Summarise the data

```{r}
dat.ros <- readRDS(here("data/lines.rds")) 
dat.weather <- readRDS(here("data/weatherhourly.rds"))

#select relevant ROS variable - ros and lineid, which matches lineid in weather data
dat.ros <- dat.ros %>% 
  select(lineid,ros=ros_kmh,
         start_time_utc,
         end_time_utc)

#select weather variables
#mean refers to mean of all barra cells that intersect the spread line.
dat.weather <- dat.weather %>% 
  select(lineid,
         wind=sfcWind_mean,
         rh=hurs_mean,
         time_utc_hourly
  ) %>% 
  
  mutate(wind=wind*3.6)


#weather is sampled hourly for the spread time of each line
#add variable to identify if BARRA sample is before, during or after spread time
dat.weather <- dat.weather %>% 
  left_join(dat.ros %>% select(lineid,start_time_utc,end_time_utc)) %>% 
  
  #round start and end times, to match barra times
  mutate(start_time_utc_round=lubridate::round_date(start_time_utc,unit = "hours"),
         end_time_utc_round=lubridate::round_date(end_time_utc,unit = "hours")) %>% 
  
  
  #remove any sampled barra weather from after end time
  filter(time_utc_hourly <= end_time_utc_round) %>% 
  
  #calculate time difference between barra sample time and start time
  #this may be useful to know, for example, wind speed 1 hour vs 3 hours before spread
  mutate(diff_start_hour=as.numeric(difftime(time_utc_hourly,start_time_utc_round,units = "hours"))) %>% 
  
  
  select(lineid,wind,rh,diff_start_hour)


#plot some of the weather and ros


dat.ros %>% 
  ggplot() +
  geom_histogram(aes(x=ros))

sample.ids <- sample(1:nrow(dat.ros), size = 9, replace = TRUE)
dat.weather %>% 
  filter(lineid %in% sample.ids) %>% 
  ggplot()+
  geom_histogram(aes(x=wind))+
  facet_wrap(~lineid)



```

### Example 1

Page 26 of workshop PDF.

You are told: "The windspeed is between 5 and 25 km/h, and temperature is between 20 and 40 degrees"

A Stan model that represents this statement...

```{r}

model_code_1 <- "
parameters {
  real<lower=5, upper=25> windspeed;
  real<lower=20, upper=40>  temperature;
}
"

```


Even though this is a minimal model, we can compile it and derive posterior samples of the single parameter...

```{r}

# Write the model code to file
f <- write_stan_file(model_code_1, dir = MODEL_DIR, basename = "model_1")

# Compile the model (takes a minute)
model_1 <- cmdstan_model(f)

# Sample the model (this will be fast!)
samples_1 <- model_1$sample(
  data = list(),
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 0
)

```


Inspect the samples.

```{r}

samples_1$summary()
post <- posterior::as_draws_df( samples_1$draws() )
plot(post$windspeed, xlab="sample", ylab="windspeed")
hist(post$temperature, xlab="temperature", ylab="density", breaks = 20)

```

Both of the above graphs show that the temperature and wind speed values have been sampled from uniform distributions over the min and max constraints defined in the model.


### Example 2

Lets say average wind speed is 9 kmh

A Stan model that represents this statement. We arbitrarily set the standard deviation to 2.0 which should give a central 90% range of approximately 6 - 12 km/h for the sampled values.

```{r}

model_code_2 <- "
parameters {
  real<lower=0, upper=20> windspeed;
}
model {
  windspeed ~ normal(9, 2);
}
"

```


This time we will just use the helper function (defined at the top of this document) to compile and sample from the model.

```{r}

post2 <- get_samples(model_code_2)

```



```{r}

mu <- mean(post2$windspeed) |> round(2)
q90 <- quantile(post2$windspeed, c(0.05, 0.95)) |> round(2)

plot(density(post2$windspeed, from=0, adj = 1), 
     main = sprintf("Mean windspeed: %g\n90%% range: %g - %g", mu, q90[1], q90[2]), 
     xlab = "Windspeed")

```



### Example 3

You are given some data for observed wind speed and asked to estimate the average.

For simplicity we will initially fit a normal distribution to the values.

A Stan model for this task...


```{r}

model_code_3 <- "
data {
  int N;  // number of observations
  array[N] real<lower=0> y;  // observed values
}
parameters {
  // Unknown average value of windspeed
  real avg_windspeed;
  
  // Unknown standard deviation of the (assumed) normal distribution
  real<lower=0> sigma;
}
model {
  // Relate the observed values to the unknown (to be estimated)
  // average windspeed
  // Each value of y is assumed to come from this distribution
  sigma ~ exponential(1);
  y ~ normal(avg_windspeed, sigma);
}
"

```



```{r}
dat.test <- dat.weather %>% 
  filter(lineid <=10)

N=nrow(dat.test)
y=dat.test$wind


post3 <- get_samples(
  model_code_3,
  data=list(N=N, y=y)
)

```

```{r}

mu <- round(mean(post3$avg_windspeed), digits = 2)
q <- round(quantile(post3$avg_windspeed, c(0.05, 0.95)), digits = 2)

dat_gg <- as.data.frame(post3) %>%
  select(avg_windspeed, sigma) %>%
  tidyr::pivot_longer(everything(), names_to = "param", values_to = "value")

ggplot(dat_gg, aes(x = value)) +
  geom_density(fill = "grey90") +
  facet_wrap(~param, scales = "free_x") +
  labs(title = sprintf("Average windspeed: mean estimate: %g\n90%% range: %g - %g", mu, q[1], q[2]))

```



### Example 4

You are given some data for windspeed observed at separate fire spread lines and asked to estimate the average for each location.

A Stan model for this task that naively treats the weather values as being drawn from a normal distribution. To make the model a little more interesting we will assume that weather is not independent across the fire spread lines, and express this by assuming that the wind speed values are drawn from normal distributions having a separate mean for each spread line but a common standard deviation (i.e. shrinkage across the lines). 


```{r}

model_code_4 <- "
data {
  int N;  // total number of observations
  int M;  // number of spread lines
  array[N] real windspeed;  // observed values
  array[N] int<lower=1, upper=M> loc;  // spread line index for each observation
}
parameters {
  // Average wind speed at each spread line
  vector[M] mu;
  
  // Common standard deviation across all spread lines
  real<lower=0> sigma;
}
model {
  // Prior for the common standard deviation with a mean of 5.0
  sigma ~ exponential(0.2); 
  
  for (i in 1:N) {
    windspeed[i] ~ normal(mu[loc[i]], sigma);
  }
}
"

```


Gather the data and fit the model.

```{r}
dat.test <- dat.weather %>% 
  filter(lineid <=10) %>% 
  #give new lineid to ensure lineids start at 1
  mutate(newlineid = as.integer(factor(lineid)) )


model4_data <- list(
  N = nrow(dat.test),
  M = max(dat.test$newlineid),
  loc = dat.test$newlineid,
  windspeed = dat.test$wind
)


post4 <- get_samples(
  model_code_4,
  data = model4_data
)


```




```{r}

dat_gg <- as.data.frame(post4) %>% 
  select(-lp__) %>%
  
  tidyr::pivot_longer(starts_with("mu"), names_to = "line", values_to = "mean_wind") %>%
  
  # get the integer line ID from the param names (mu.1, mu.2 etc.) and
  # format it for facet labels
  mutate(line = as.integer(str_extract(line, "\\d+") ),
         line = sprintf("line %02d", line))

# Observed data to display as a rug plot in each facet
dat_gg_observed <- dat.test %>%
  select(line = newlineid, wind) %>%
  mutate(line = sprintf("line %02d", line))

# Empirical mean of the observed wind speed value for each line
dat_gg_means <- dat_gg_observed %>%
  group_by(line) %>%
  summarize(obs_mean = mean(wind))


ggplot(dat_gg, aes(x = mean_wind)) +
  geom_density(fill = "skyblue") +
  
  # Observed values
  geom_rug(data = dat_gg_observed, aes(x = wind), 
           colour = "blue", size = 1) +
  
  # Empirical mean values
  geom_vline(data = dat_gg_means, aes(xintercept = obs_mean), 
             colour = "blue", linetype = "dashed") +
  
  facet_wrap(~line)

```

The graph shows that the model has fitted the distribution of mean wind value for each line such that it is centred on the mean of the observed values. 


###Example 4a

Next we do a very similar model but this time for relative humidity (RH) rather than wind speed. RH values are percentages, i.e. bounded values, so its best to honour this by fitting an appropriate distribution. Since percentage values are really just proportions, the beta distribution is an obvious choice. Note that the beta distribution supports values between, but not including, 0 and 1. The data does not contain any RH values of 0% or 100% so we are okay.

A bit of detail...

A beta distribution is commonly described via two parameters, variously called `a` and `b`, or `alpha` and `beta`, or (in R) `shape1` and `shape2`. 

The mean of a beta(a, b) distribution is given by `mu = a / (a+b)`.

Here are some examples of beta distributions that all have a mean of 0.4...

```{r}

dat <- data.frame(
  a = c(10, 4, 2, 1.5), 
  b = c(15, 6, 3, 2.25),
  clrs <- c("blue", "steelblue", "mediumpurple", "darkred")
)

for (i in 1:nrow(dat)) {
  curve(dbeta(x, shape1 = dat$a[i], shape2 = dat$b[i]), 
        xlab = "Value",
        ylab = "Density",
        col = dat$clrs[i], 
        lwd = 2,
        add = i>1)
}

```


It is often a bit more convenient to describe a beta distribution more directly in terms of its mean $\mu$ and a dispersion or spread parameter $\phi$. The relationship to the standard beta parameters $\alpha$ and $\beta$ is:

$$
\alpha = \mu \phi \\
\beta = (1 - \mu)\phi
$$


In JAGS you would set priors for the mean and dispersion parameter, then calculate the correspond a and b values for the `dbeta(a, b)` distribution. In Stan, things are a little easier since there is a `beta_proportion` distribution that is parameterized directly via the mean and dispersion.

Note: we are being very lazy here and not bothering to define priors for either the mean RH values (`mu`) or the common dispersion parameter (`phi`). Instead we are setting constraints on the range of these parameters and allowing Stan to sample a corresponding uniform distribution for each one.

```{r}

model_code_4a <- "
data {
  int N;  // total number of observations
  int M;  // number of spread lines
  array[N] real<lower=0, upper=1> rh;  // observed RH values as proportions
  array[N] int<lower=1, upper=M> loc;  // spread line index for each observation
}
parameters {
  // Average RH value at each spread line
  array[M] real<lower=0.01, upper=0.9> mu;
  
  // Common dispersion parameter for the beta distributions across all spread lines
  real<lower=0, upper=50> phi;
}
model {
  for (i in 1:N) {
    rh[i] ~ beta_proportion(mu[loc[i]], phi + 1);
  }
}
"

```


Gather the data and fit the model.

```{r}

dat.test <- dat.weather %>% 
  filter(lineid <=10) %>% 
  #give new lineid to ensure lineids start at 1
  mutate(newlineid = as.integer(factor(lineid)) )


model4a_data <- list(
  N = nrow(dat.test),
  M = max(dat.test$newlineid),
  loc = dat.test$newlineid,
  
  # Express RH values as proportions
  rh = dat.test$rh / 100  
)


post4a <- get_samples(
  model_code_4a,
  data = model4a_data
)


```


```{r}

dat_gg <- as.data.frame(post4a) %>% 
  select(-lp__) %>%
  
  tidyr::pivot_longer(starts_with("mu"), names_to = "line", values_to = "mean_rh") %>%
  
  # get the integer line ID from the param names (mu.1, mu.2 etc.) and
  # format it for facet labels
  mutate(line = as.integer(str_extract(line, "\\d+") ),
         line = sprintf("line %02d", line))

# Observed data to display as a rug plot in each facet
dat_gg_observed <- dat.test %>%
  select(line = newlineid, rh) %>%
  mutate(line = sprintf("line %02d", line),
         rh = rh / 100)

# Empirical mean of the observed RH value for each line
dat_gg_means <- dat_gg_observed %>%
  group_by(line) %>%
  summarize(obs_mean = mean(rh))


ggplot(dat_gg, aes(x = mean_rh)) +
  geom_density(fill = "skyblue") +
  
  # Observed values
  geom_rug(data = dat_gg_observed, aes(x = rh),
           colour = "blue", size = 1) +
  
  # Empirical mean values
  geom_vline(data = dat_gg_means, aes(xintercept = obs_mean),
             colour = "blue", linetype = "dashed") +
  
  scale_x_continuous(labels = scales::percent) +
  
  facet_wrap(~line)

```

Note that, unlike the mean wind speed distributions earlier, not all of these mean RH distributions are centred on the empirical mean value for the respective fire spread line, but most of them are and the others are pretty close. This happens because the beta distributions fitted to the RH values will be a little right-skewed.


### Basic regression model relating rate of spread to wind speed and RH

This is a very simplified model of ROS values in relation to wind speed and RH. Its purpose is to experiment with different ways of handling the data where there is a single ROS value but multiple weather values for each fire spread line. 

It will probably take 5-10 minutes to fit the model when using the full data set.


```{r}

# Weather data for fire spread lines
dat <- dat.weather %>%
  filter(diff_start_hour > -2)

model5_data <- list(
  W = nrow(dat),
  L = max(dat$lineid),
  wline_id = dat$lineid,  # line ID for weather observations
  wind = dat$wind,
  rh = dat$rh / 100
)

# Rate of spread values for lines
model5_data <- c(model5_data, 
                 list(ros_line_id = dat.ros$lineid, ros = dat.ros$ros))

# Check that the weather line IDs and the ROS line IDs line up properly
MaxLineID <- max(model5_data$wline_id)

# Check there are no gaps in the line IDs
stopifnot( all(model5_data$wline_id %in% seq_len(MaxLineID)) )
  
# Check that all weather line IDs are in the ROS line IDs and vice versa
stopifnot( length(symdiff(model5_data$wline_id, model5_data$ros_line_id)) == 0)

```


Fit the model. Note - this will probably take 5-10 minutes.

```{r}

model_code_5 <- "
data {
  int<lower=1> W;     // number of weather observations
  int<lower=1> L;     // number of fire spread lines
  
  array[W] int<lower=1> wline_id;  // line ID for each weather observation
  vector<lower=0>[W] wind;         // wind speed values
  vector<lower=0>[W] rh;      // relative humidity values expressed as proportions
  vector<lower=0>[L] ros;   // outcome - rate of spread
}
transformed data {
  vector[L] ros_log = log(ros);
}
parameters {
  vector[L] avg_wind;  // imputed wind value for each spread line
  real<lower=0> sigma_wind;
  
  vector<lower=0.01, upper=0.9>[L] avg_rh;    // imputed relative humidity value for each line
  real<lower=0, upper=50> phi_rh;
  
  real alpha;           // intercept - could define weakly informative priors in model block
  real beta_wind;       // coefficients for predictors
  real beta_rh;       // coefficients for predictors
  real<lower=0> sigma_ros;  // error scale
}
model {
  vector[L] mu;        // linear predictor value for ROS for each line
  
  for (i in 1:W) {
    wind[i] ~ normal(avg_wind[wline_id[i]], sigma_wind);
    rh[i] ~ beta_proportion(avg_rh[wline_id[i]], phi_rh + 1);
  }
  
  for (i in 1:L) {
    mu[i] = alpha + beta_wind * avg_wind[i] + beta_rh * avg_rh[i];
    ros_log[i] ~ normal(mu[i], sigma_ros);
  }
}"

```


Fit the model and just retrieve the posterior samples for the parameters that we need, i.e. the regression coefficients and not the many imputed values for wind and RH.

Note: You can safely ignore one or two warnings about rejected samples.

```{r}

post5 <- get_samples(
  model_code_5,
  data = model5_data,
  vars = c("alpha", "beta_wind", "beta_rh", "sigma_ros"),
  refresh = 100
)

```



```{r}

dat_post5 <- as.data.frame(post5)

# Wind and RH values for predictions
dat_pred <- expand.grid(
  wind = c(10, 20, 30, 40),
  rh = c(20, 50, 80) / 100
)


ros_pred <- dat_post5 %>%
  # Add prediction data to every posterior record
  cross_join(dat_pred) %>%
  
  # Predict ROS values
  mutate(mu = alpha + beta_wind * wind + beta_rh * rh,
         ros_log = rnorm(n(), mu, sigma_ros),
         ros = exp(ros_log)) %>%
  
  # Summary statistics
  group_by(wind, rh) %>%
  summarize(lwr90 = quantile(ros, 0.05),
            lwr50 = quantile(ros, 0.25),
            mid = quantile(ros, 0.5),
            upr50 = quantile(ros, 0.75),
            upr90 = quantile(ros, 0.95) )

```


Take a look at the predictions...

```{r}

dodgew <- 5

dat_gg <- ros_pred %>%
  mutate(frh = factor(paste0(rh * 100, "%")))


dat_obs <- dat.weather %>%
  filter(diff_start_hour > -2) %>%
  group_by(lineid) %>%
  summarize(wind = mean(wind), rh = mean(rh)) %>%
  
  left_join(dat.ros %>% select(lineid, ros), by = "lineid")


ggplot(data = dat_gg, aes(x = wind)) +
  
  # Plot points for observed ROS values vs mean wind speed, just for
  # visual comparison to the predicted ROS ranges
  geom_point(data = dat_obs, aes(x = wind, y = ros), alpha = 0.1) +

  
  geom_linerange(aes(ymin = lwr90, ymax = upr90, colour = frh),
                 position = position_dodge(width = dodgew),
                 linewidth = 1) +
  
  geom_linerange(aes(ymin = lwr50, ymax = upr50, colour = frh),
                 position = position_dodge(width = dodgew),
                 linewidth = 2.5, size = 1.5) +
  
  labs(colour = "Relative humidity",
       x = "Wind speed (km/h)",
       y = "Predicted ROS")  

```





## Michael's earlier code from this point on...


```{r}
dat_gg <- as.data.frame(post5) %>% 
  select(-lp__) 


dat_gg %>% 
  tidyr::pivot_longer(everything()) %>% 
  ggplot(aes(x = value, colour = name)) +
  geom_density(linewidth = 1)+
  #lims(x=c(0,50))+
  facet_wrap(~name)
```

```{r}
#Create a grid of predictor values
wind_seq <- seq(0, 100, length.out = 100)
rh_mean <- mean(dat.test.2$mean_rh)


draws <- as.data.frame(post5)
# Get posterior mean estimates
beta_wnd <- draws$beta_wnd
beta_rh <- draws$beta_rh
alpha <- draws$alpha



#Generate posterior predictions for wind effect

# Use 100 draws for plotting, for speed
set.seed(123)
draw_idx <- sample(1:nrow(draws), 100)

pred_df <- expand.grid(
  wind = wind_seq,
  draw = draw_idx
) %>%
  mutate(
    alpha = alpha[draw],
    beta_wnd = beta_wnd[draw],
    beta_rh = beta_rh[draw],
    pred = alpha + beta_wnd * wind + beta_rh * rh_mean
  )





#Summarise the effect
library(tidyr)

pred_summary <- pred_df %>%
  group_by(wind) %>%
  summarise(
    mean = mean(pred),
    lower = quantile(pred, 0.05),
    upper = quantile(pred, 0.95)
  )

#plot
ggplot(pred_summary, aes(x = wind, y = mean)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  labs(
    x = "Wind Speed",
    y = "Predicted ROS",
    title = "Marginal Effect of Wind on ROS (RH fixed at mean)"
  )

```



